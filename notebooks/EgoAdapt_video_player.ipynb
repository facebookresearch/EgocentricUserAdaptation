{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cb4aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Video player of the EgoAdapt samples (2.1s fragments) in the user video streams.\n",
    "Per sample the action label and meta-data is displayed.\n",
    "\n",
    "Before running this experiment, make sure you have run the Stream meta-data collector (src/continual_ego4d/processing/run_summarize_user_streams.py) and set the resulting paths in this notebook.\n",
    "\"\"\"\n",
    "import json\n",
    "import os.path as osp\n",
    "import pickle\n",
    "\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from IPython.display import display, Image, clear_output\n",
    "from tqdm import tqdm\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "pd.set_option('display.max_rows', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08976e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CONFIG: Add your config params here\n",
    "\"\"\"\n",
    "TRAIN_MODE = 'train'\n",
    "\n",
    "if TRAIN_MODE == 'train': # Path obtained by Stream meta-data collector (src/continual_ego4d/processing/run_summarize_user_streams.py)\n",
    "    train_usersummary_file_nooverlap_include_videpaths = \"/your/path/to/logs/2022-09-06_18-43-43_UIDc4605fd4-8f70-4fcd-bee2-e16b5dd22820/dataset_entries_train_ego4d_LTA_train_usersplit_10users.ckpt\"\n",
    "    FILE_TO_ANALYZE = train_usersummary_file_nooverlap_include_videpaths\n",
    "\n",
    "elif TRAIN_MODE == 'test': # Path obtained by Stream meta-data collector (src/continual_ego4d/processing/run_summarize_user_streams.py)\n",
    "    test_usersummary_file_nooverlap_include_videpaths=\"/your/path/to/logs/2022-10-07_04-33-34_UIDd679068a-dc6e-40ff-b146-70ffe0671a97/dataset_entries_test_ego4d_LTA_test_usersplit_40users.ckpt\"\n",
    "    FILE_TO_ANALYZE = test_usersummary_file_nooverlap_include_videpaths\n",
    "\n",
    "elif TRAIN_MODE == 'pretrain': # Path to direct JSON from pretraining\n",
    "    pretrain_unsegmented_json = '/your/path/to/2022-09-08_17-17-16_ego4d_LTA_usersplit/ego4d_LTA_pretrain_incl_nanusers_usersplit_148users.json'\n",
    "    FILE_TO_ANALYZE = pretrain_unsegmented_json\n",
    "    video_parent_path =\"../data/Ego4D/v1/clips\" # Parent dir of videos\n",
    "\n",
    "# Train/test are segmented in 2s clips for online learning\n",
    "if TRAIN_MODE in ['train','test']: # Pickle\n",
    "    with open(FILE_TO_ANALYZE, 'rb') as f:\n",
    "        ds = pickle.load(f)\n",
    "    \n",
    "    CUSTOM_VIDEO_PLAY_LENGTH_SEC = None # Play the start-end 2s\n",
    "    clip_start_key = 'clip_start_sec'\n",
    "    clip_end_key = 'clip_end_sec'\n",
    "    user_key = 'user_id'\n",
    "    scenarios_key = 'parent_video_scenarios'\n",
    "\n",
    "    def video_path_fetch_fn(entry):\n",
    "        return entry['video_path']\n",
    "\n",
    "# Pretrain uses original ego4d action-annotations\n",
    "else: # JSON\n",
    "    with open(pretrain_unsegmented_json, 'r') as f:\n",
    "        ds = json.load(f)['users']\n",
    "    \n",
    "    CUSTOM_VIDEO_PLAY_LENGTH_SEC = None\n",
    "    clip_start_key = 'action_clip_start_sec'\n",
    "    clip_end_key = 'action_clip_end_sec'\n",
    "    user_key = 'fb_participant_id'\n",
    "    scenarios_key = 'parent_video_scenarios'\n",
    "    \n",
    "    def video_path_fetch_fn(entry):\n",
    "        return osp.join(video_parent_path,f'{entry[\"clip_uid\"]}.mp4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "List all users available\n",
    "\"\"\"\n",
    "all_users = list(ds.keys())\n",
    "all_users"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\" Select a user. \"\"\"\n",
    "SELECTED_USER = '16'\n",
    "\n",
    "\"\"\"Single entry looks like:\"\"\"\n",
    "ds[SELECTED_USER][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3499c725",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Run the video player \"\"\"\n",
    "\n",
    "def draw_label(img, text, pos, bg_color):\n",
    "    font_face = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    scale = 0.8\n",
    "    color = (255, 0, 0)\n",
    "    thickness = cv2.FILLED\n",
    "    margin = 2\n",
    "    txt_size = cv2.getTextSize(text, font_face, scale, thickness)\n",
    "\n",
    "    end_x = pos[0] + txt_size[0][0] + margin\n",
    "    end_y = pos[1] - txt_size[0][1] - margin\n",
    "\n",
    "    cv2.rectangle(img, pos, (end_x, end_y), bg_color, thickness)\n",
    "    cv2.putText(img, text, pos, font_face, scale, color, 1, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "user_sequence = ds[SELECTED_USER]\n",
    "pbar = tqdm(total=len(user_sequence))\n",
    "\n",
    "initial = True\n",
    "entry_idx = 0\n",
    "cmd = None\n",
    "\n",
    "\n",
    "while cmd != 'q':\n",
    "    \n",
    "    if initial:\n",
    "        cmd='r' # Replay first instance\n",
    "        initial=False\n",
    "    else:\n",
    "        cmd = input()\n",
    "    \n",
    "    try:\n",
    "        cmd = int(cmd)\n",
    "        entry_idx = cmd\n",
    "        print(f\"Playing from annotation number entry idx: {entry_idx}\")\n",
    "    except:\n",
    "        if cmd == 'n':\n",
    "            entry_idx+=1\n",
    "            print(f'Playing next video:{entry_idx}')\n",
    "        elif cmd == 'p':\n",
    "            entry_idx-=1\n",
    "            print(f'Playing previous video:{entry_idx}')\n",
    "        elif cmd == 'r':\n",
    "            print(f'Replaying video {entry_idx}')\n",
    "        elif cmd == 'q':\n",
    "            print('Quiting video watcher')\n",
    "            break\n",
    "        else:\n",
    "            raise ValueError('cmd not recognized')\n",
    "    \n",
    "    # Overwrite previous output of video and label\n",
    "    pbar.reset()\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    entry = user_sequence[entry_idx]\n",
    "    video_path = video_path_fetch_fn(entry)\n",
    "    clip_start_msec = entry[clip_start_key] * 1000\n",
    "    if CUSTOM_VIDEO_PLAY_LENGTH_SEC is None:\n",
    "        clip_end_msec = entry[clip_end_key] * 1000\n",
    "    else:\n",
    "        clip_end_msec = clip_start_msec + CUSTOM_VIDEO_PLAY_LENGTH_SEC * 1000\n",
    "    print(f\"Fetching video: {video_path}\")\n",
    "    \n",
    "    info_str = \"(entry {}) USER {}: {:.1f}s-{:.1f}s, action_idx={}\".format(\n",
    "        entry_idx,\n",
    "        entry[user_key],\n",
    "        entry[clip_start_key],\n",
    "        clip_end_msec/1000,\n",
    "        entry['action_idx'],\n",
    "\n",
    "    )\n",
    "    print(info_str)\n",
    "    print(\"LABEL\\t{}-{}\".format(entry['verb'],entry['noun'],))\n",
    "    print(\"SCENARIO\\t{}\".format(entry[scenarios_key], ))\n",
    "    print(f\"\\nuser{entry[user_key]}_t{entry_idx}_{entry['verb']}_{entry['noun']}\")\n",
    "    pbar.update(entry_idx)\n",
    "    pbar.refresh()\n",
    "    \n",
    "    # Video\n",
    "    # See API to set video start/end times: https://docs.opencv.org/3.4/d4/d15/group__videoio__flags__base.html#gaeb8dd9c89c10a5c63c139bf7c4f5704d\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    video.set(cv2.CAP_PROP_POS_MSEC, clip_start_msec)\n",
    "    display_handle=display(None, display_id=True)\n",
    "    \n",
    "    try:\n",
    "        while video.get(cv2.CAP_PROP_POS_MSEC) <= clip_end_msec:\n",
    "            _, frame = video.read()\n",
    "            _, frame = cv2.imencode('.jpeg', frame)\n",
    "    \n",
    "            draw_label(frame,\"HELLOW\", (50,50),(255,0,0))\n",
    "            display_handle.update(Image(data=frame.tobytes()))\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "    finally:\n",
    "        video.release()\n",
    "\n",
    "display_handle.update(None)\n",
    "pbar.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}